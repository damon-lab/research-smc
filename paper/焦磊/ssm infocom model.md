MODELS AND PROBLEM FORMULATION

**A. System Models**

**Edge system and ML tasks:** We consider an edge computing system that consists of distributed and heterogeneous edge severs, whose resource is virtualized into multiple containers (e.g., Docker). The contains are running a set $\mathcal{M}=\{1,2, \ldots, M\}$ of training models and a set $\mathcal{N}=\{1,2, \ldots, N\}$ of inference models. We study our system with a series of consecutive time slots $\mathcal{T}=\{1,2,\ldots,T\}$. 

For Machine Learning (ML) tasks, the eages is considered to process the training and inference workloads among the specified time slots $\mathcal{T}$. Without loss of generality, we consider the training workloads have been stored in advance at the edge , denoted as $T_m$. For each kind of inference workloads that is denoted as $I_{n,t},n\in \mathcal{N}$, we consider the inference workloads arrive dynamically and must be processed at each time slot $t$. Considering that edges generally deploy lightweight deep learning algorithms, the inference workload $n \in \mathcal{N}$ can be processed by various inference algorithms that are denoted as $\mathcal{K}=\{1,2, \ldots, K\}$, which are generated by different model compression methods. Our goal is to maximize inference accuracy while balancing the carbon footprint and it is equivalent to minimize inference accuracy loss $A_{n,k}, \forall n \in \mathcal{N}, k \in \mathcal{K}$, defined  as 1 minus its percentage accuracy.

**Control Variables:** We now elaborate the control decisions that we tune to optimize the long-term inference accuracy considering the carbon emission. We use $y_{n,k,t}, \forall n \in \mathcal{N}, k \in \mathcal{K}, t \in \mathcal{T}$ to represent whether the inference workload $n$ is processed $(y_{n,k,t}=1)$ or not $(y_{n,k,t}=0)$ by the inference algorithm $k$. We denote by $x_{m,t}, \forall m \in \mathcal{M}, t \in \mathcal{T}$ the number of training workload trained at time $t$.

**Resource cost of ML:** As for the computing cost, we denote the resource cost (e.g., in computer memory and CPU usage) for a single training instance as $R_m$, the resource cost for a single inference instance processed by algorithm $k$ as $Z_{n,k}$. It is necessary to clarify that in order to better focus the carbon footprint generated by machine learning tasks and to facilitate model construction, we do not consider here the CPU interactions due to the execution of different containers. The total resource cost of ML at $t$ consists of two parts, i.e., the cost of trainning workloads, plus the cost of inferencing workloads. Thus, we have

$\textstyle\sum\nolimits_{m} x_{m,t} R_{m}+\sum\nolimits_{n} \sum\nolimits_{k} I_{n,t} y_{n,k,t} Z_{n,k}$

**Carbon Emission:** In this paper, we focus on leveraging the delay-tolerance as well as the temporal variability of the electricity grid power's carbon emission intensity to carefully trade-off inference accuracy for carbon reduction. Moreover, the composition of the energy mix of the grid will vary at different moments in time, so the unit carbon emissions from energy consumption at different locations at different time will vary too\cite{bashir2021enabling}. And we use $c_t$ to denote the  time-varying parameters of carbon emission intensity of grid power, which is obtained by the temporal variabilities of carbon emission intensity and renewable energy availability. 

The total carbon emission of ML at $t$ consists of two parts, i.e., the emission of processing trainning workloads, plus the emission of processing inferencing workloads. We calculate the carbon emissions generated by performing the machine learning task in the manner of multiplying the energy consumption with the carbon emission intensity, which is consistent with the objective rule that the higher the energy consumption the higher the carbon emissions at the same moment. We denote the energy consumption for training a single training instance as $P_m$ and the energy consumption for processing a single inference instance by algorithm $k$ as $Q_{n,k}$. Thus, we have

$[\sum\nolimits_{m}x_{m,t} P_{m}+\sum\nolimits_{n} \sum\nolimits_{k} I_{n,t} y_{n,k,t} Q_{n,k}] \cdot c_{t}$

Without loss of generality, we introduce carbon offsets denoted as $R_t$ because the edge side may exceed the carbon emission limit due to more training tasks and inference requests from users, and needs to purchase a certain amount of carbon offsets to offset. Thus, the total carbon emission of ML at $t$ is

$[\sum\nolimits_{m}x_{m,t} P_{m}+\sum\nolimits_{n} \sum\nolimits_{k} I_{n,t} y_{n,k,t} Q_{n,k}] \cdot c_{t}-R_{t}$

**B. Problem Formulation, Challenges, and Goal**

与云数据中心相比，边缘服务器本质上具有有限的计算能力[14]，我们将其重新表述为最大任务服务或赋予的计算资源，具体取决于用户的卸载需求。

当用户需要同构的资源(内存、CPU、存储)来处理他们卸载的任务时，服务器j的任务服务能力Mj表示它可以同时处理多少任务。如果运算量过大，服务器会随机选择Mj任务，其余任务会因为能力有限而放弃。一旦丢弃，用户经历高延迟并观察到零奖赏j(T)=0。假设M=∑K j=1 Mj≥N，即MEC系统有足够的资源来满足所有的卸载请求。

随时到达的推理任务和

为什么不考虑动态更新训练模型？

我们不考虑每次动态更新模型，因为容器重新加载模型所需要的冷启动事件比运行一个实例要多的多，将会很影响用户体验。

为什么不考虑动态到达的训练负载？

边缘服务器的计算能力相当有限，若同时考虑动态的训练和推理负载，则当大量的训练任务和推理任务到达时，服务器可能无法及时处理完所有推理任务，这与用户的需求是相悖的。

为什么不考虑边缘网络的损耗？

因为网络的损耗与执行ai任务的损耗相比不值一提。