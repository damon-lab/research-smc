# AI in 5G: The Case of Online Distributed Transfer Learning over Edge Networks

论文主要解决边缘网络下的迁移学习问题，将分布式转移学习表述为一个长期成本优化的非线性混合整数模型，并设计了多项式时间的在线算法。在协调模型部署、数据调度和推理汇总的同时，我们的方法通过结合现有的离线模型和正在训练的在线模型，使用基于推理动态到达的数据样本自适应更新的权重，产生新的模型。

随着时间的推移，5G人工智能所存在的问题：

- 实际运行中数据的特征和标签之间存在”漂移“
- 不同地区的基础数据分布可能不同
- 迁移学习：当现有的人工智能模型变得不那么准确时，与其放弃它们并从头开始建立新的模型，不如利用现有的模型来帮助建立新的模型，即从现有的模型（即离线分类器）和正在训练的模型（即在线分类器）转移 "知识 "到新的模型，而新的模型是它们的组合，特别是当新的数据本身可能不足以进行训练或当基础数据分布可能拥有周期性的模式
  - 以分布式的方式设计和实现迁移学习不难，从异构的离线分类器提取只是，在动态到达的数据样本上训练在线分类器，需要做出一套全面的控制决策，例如分类器放置、数据样本调度、推理聚合和训练参数更新。同时要确保正在产生的新分类器的质量或准确性。
  - 以在线方式使分布式转移学习具有成本效益并不容易，需要考虑未知变化的边缘的运行成本、边缘之间的延迟以及每个边缘的可用容量，**要求在不观察任何未来输入的情况下控制系统，以追求长期优化**
  - **选择边缘从云端下载离线分类器或托管要创建的在线分类器的决策是时间耦合的**

当前研究没有考虑迁移学习的资源或成本开销，以及分布式云-边环境或在线方式，论文主要从以下几个方面进行优化

- 论文将边缘网络上的分布式迁移学习建模并表述为一个长期的成本优化问题，包含了托管离线和在线分类器的运行成本、下载离线分类器和实例化本地环境的启动成本、调度数据样本和汇总推理结果的延迟，以及应用组合分类器对数据样本进行推理所产生的错误数量
- 设计了一套多项式时间的在线算法来解决上述问题
  - 首先设计了一个基于primal-dual的算法1，以解决一次性问题，将离线分类器与数据调度和推理聚合放在一起，假设所有其他控制决策已经做出
  - 随后设计了算法2来平衡来自算法1的新决定与之前的离线分类器放置的最新决定，假设在线分类器放置是给定的。通过实时比较切换到这个新位置的当前启动成本与继续留在以前位置的累计非启动成本，来决定是否切换到新位置
  - 设计了算法3，以确定在线分类器的位置，以及与算法2的决策相关的成本
  - 有了每个分类器的权重，进一步设计了算法4，在每个时隙，调用算法3来设置分布式迁移学习，减少那些在动态到达的数据样本上推断错误方面准确性差的分类器的权重，并根据它所造成的损失来更新在线分类器。每个数据样本的最终综合分类器是现有离线分类器和正在该数据样本上训练的在线分类器的加权和。

